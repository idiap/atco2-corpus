{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba284335-79e8-4c2b-8617-913bd91b4515",
   "metadata": {},
   "source": [
    "# **Use our fine-tuned XLSR-Wav2Vec2 on Air Traffic Control data with ðŸ¤— Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7425adf4-9941-4746-ac8d-b552438b2d41",
   "metadata": {},
   "source": [
    "Wav2Vec2 is a pretrained model for Automatic Speech Recognition (ASR) and was released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) by Alexei Baevski, Michael Auli, and Alex Conneau.  Soon after the superior performance of Wav2Vec2 was demonstrated on the English ASR dataset LibriSpeech, *Facebook AI* presented XLSR-Wav2Vec2 (click [here](https://arxiv.org/abs/2006.13979)). XLSR stands for *cross-lingual  speech representations* and refers to XLSR-Wav2Vec2's ability to learn speech representations that are useful across multiple languages.\n",
    "\n",
    "Similar to Wav2Vec2, XLSR-Wav2Vec2 learns powerful speech representations from hundreds of thousands of hours of speech in more than 50 languages of unlabeled speech. Similar, to [BERT's masked language modeling](http://jalammar.github.io/illustrated-bert/), the model learns contextualized speech representations by randomly masking feature vectors before passing them to a transformer network.\n",
    "\n",
    "![wav2vec2_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/xlsr_wav2vec2.png)\n",
    "\n",
    "The authors show for the first time that massively pretraining an ASR model on cross-lingual unlabeled speech data, followed by language-specific fine-tuning on very little labeled data achieves state-of-the-art results. See Table 1-5 of the official [paper](https://arxiv.org/pdf/2006.13979.pdf).\n",
    "\n",
    "(**Introduction from** [Google Colab of Patrick von Platen](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_XLSR_Wav2Vec2_on_Turkish_ASR_with_%F0%9F%A4%97_Transformers.ipynb?authuser=1#scrollTo=V7YOT2mnUiea) implementation.])\n",
    "\n",
    "\n",
    "Our fine-tuned model is open-sourced at: https://huggingface.co/Jzuluaga/wav2vec2-xls-r-300m-en-atc-atcosim and https://huggingface.co/Jzuluaga/wav2vec2-large-960h-lv60-self-en-atc-atcosim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7762f07-a1fe-4314-9296-9d55c689dcbf",
   "metadata": {},
   "source": [
    "In this notebook, we will give you an initial explanation of how to use the XLSR-Wav2Vec2's fine-tuned checkpoint on Air Traffic Control Data, with and without using language model. If you use a LM you can achieve better results. In case of interest, please follow this notebook: [Boosting Wav2Vec2 with n-grams in ðŸ¤— Transformers](https://huggingface.co/blog/wav2vec2-with-ngram).\n",
    "\n",
    "- We also have an explained setup on how to train a LM with KenLM and integrate it on this model in our GitHub repository: [How Does Pre-trained Wav2Vec 2.0 Perform on Domain Shifted ASR? An Extensive Benchmark on Air Traffic Control Communications](https://github.com/idiap/w2v2-air-traffic)\n",
    "\n",
    "For demonstration purposes, we fine-tune the [wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on the low resource ATC dataset:\n",
    "\n",
    "- ATCOSIM ASR dataset: more information in their [ATCOSIM website](https://www.spsc.tugraz.at/databases-and-tools/atcosim-air-traffic-control-simulation-speech-corpus.html)\n",
    "- However, **do not worry**, we have prepared the database in the [Datasets](https://huggingface.co/docs/datasets/index) format, here: [ATCOSIM CORPUS on HuggingFace](https://huggingface.co/datasets/Jzuluaga/atcosim_corpus). You can scroll and check the train/test partitions, and even listen to some audios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065938a-0352-4856-a581-c89a0effc8c9",
   "metadata": {},
   "source": [
    "# Data exploration of the model - Load ATCOSIM dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c81332-f8fa-4387-9c9b-4bc236be7572",
   "metadata": {},
   "source": [
    "We need to load the dataset in HuggingFace format. The dataset is here: [ATCOSIM CORPUS on HuggingFace](https://huggingface.co/datasets/Jzuluaga/atcosim_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b8f34-464d-4ebb-8050-6f3b00554392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration Jzuluaga--atcosim_corpus-4ff78a77757fa2bd\n",
      "WARNING:datasets.builder:Found cached dataset parquet (/idiap/temp/jzuluaga/HF_CACHE/datasets/Jzuluaga___parquet/Jzuluaga--atcosim_corpus-4ff78a77757fa2bd/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, Audio\n",
    "dataset_id = \"Jzuluaga/atcosim_corpus\"\n",
    "\n",
    "# we only load the 'test' partition, however, if you want to load the 'train' partition, you can change \n",
    "atcosim_corpus_test = load_dataset(dataset_id, \"test\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227fd07f-7ac7-4454-a51c-85d4cbdf949b",
   "metadata": {},
   "source": [
    "This is a short function that displays some random samples from the dataset. Only for visualization purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e9096-c264-47ca-8b43-dce3d1e46ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191d70d-f49f-4588-9f75-1d512a2fd13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atcosim_zf1_05_147_000000_000263</td>\n",
       "      <td>contact milan one three four five two good bye</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atcosim_zf1_05_148_000000_000273</td>\n",
       "      <td>u s air one four descend to flight level two nine zero</td>\n",
       "      <td>2.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atcosim_zf1_05_152_000000_000303</td>\n",
       "      <td>air france three five six reims one three four four good bye</td>\n",
       "      <td>3.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atcosim_zf1_05_146_000000_000350</td>\n",
       "      <td>lufthansa four three nine three descend to flight level two seven zero</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atcosim_zf1_05_153_000000_000308</td>\n",
       "      <td>viva nine zero eight one descend to flight level two nine zero</td>\n",
       "      <td>3.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>atcosim_zf1_05_150_000000_000301</td>\n",
       "      <td>swiss air four eight eight climb to flight level three two zero</td>\n",
       "      <td>3.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>atcosim_zf1_05_154_000000_000344</td>\n",
       "      <td>swiss air four eight eight contact rhein one three two decimal four adieu</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>atcosim_zf1_05_155_000000_000359</td>\n",
       "      <td>air france one five five four milan one three four five two good bye</td>\n",
       "      <td>3.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>atcosim_zf1_05_151_000000_000308</td>\n",
       "      <td>air france three five six set course to morok</td>\n",
       "      <td>3.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>atcosim_zf1_05_149_000000_000270</td>\n",
       "      <td>aero lloyd five one seven set course direct to karlsruhe</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(atcosim_corpus_test.remove_columns([\"audio\", \"segment_start_time\", \"segment_end_time\"]), num_examples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0e647-f893-4d3e-ba45-941d53844f45",
   "metadata": {},
   "source": [
    "## Load Tokenizer, Feature Extractor and Model\n",
    "ASR models transcribe speech to text, which means that we both need a feature extractor that processes the speech signal to the model's input format, *e.g.* a feature vector, and a tokenizer that processes the model's output format to text. \n",
    "\n",
    "In ðŸ¤— Transformers, the XLSR-Wav2Vec2 model is thus accompanied by both a tokenizer, called [Wav2Vec2CTCTokenizer](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2ctctokenizer), and a feature extractor, called [Wav2Vec2FeatureExtractor](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2featureextractor). Here, we can also load the Model with `AutoModelForCTC` and the processors with `Wav2Vec2Processor` and `Wav2Vec2ProcessorWithLM` functions.\n",
    "\n",
    "Let's start by creating the tokenizer responsible for decoding the model's predictions.\n",
    "And we also download the model which is public here: [XLSRXLSR-Wav2Vec2 model fine-tuned on ATC data](https://huggingface.co/Jzuluaga/wav2vec2-xls-r-300m-en-atc-atcosim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70137862-0134-409b-ae0c-a26b8fff74f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/idiap/home/jzuluaga/.local/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'snapshot_download': allow_regex. Will not be supported from version '0.12'.\n",
      "\n",
      "Please use `allow_patterns` and `ignore_patterns` instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a68f8ff5254f1da663a7c2584acaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pyctcdecode.language_model:Only 0 unigrams passed as vocabulary. Is this small or artificial data?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCTC, Wav2Vec2Processor, Wav2Vec2ProcessorWithLM\n",
    "import torchaudio.functional as F\n",
    "\n",
    "# ID of the model, link: https://huggingface.co/Jzuluaga/wav2vec2-xls-r-300m-en-atc-atcosim\n",
    "model_id = \"Jzuluaga/wav2vec2-xls-r-300m-en-atc-atcosim\"\n",
    "\n",
    "# load the model, you can ignore the warnings\n",
    "model = AutoModelForCTC.from_pretrained(model_id)\n",
    "processor_with_lm = Wav2Vec2ProcessorWithLM.from_pretrained(model_id)\n",
    "processor_without_lm = Wav2Vec2Processor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a1d9c-fcd1-42fb-9283-456c8d471b52",
   "metadata": {},
   "source": [
    "## Load the sample in memory\n",
    "\n",
    "We load one sample into memory to show an example of how to run our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7ddc6ef1-5a73-40c8-8199-deb3e94bdb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "# load one sample into memory\n",
    "\n",
    "sample_iter = next(iter(atcosim_corpus_test))\n",
    "file_sampling_rate = sample['audio']['sampling_rate']\n",
    "\n",
    "if file_sampling_rate != 16000:\n",
    "    resampled_audio = F.resample(torch.tensor(sample[\"audio\"][\"array\"]), file_sampling_rate, 16000).numpy()\n",
    "else:\n",
    "    resampled_audio = torch.tensor(sample[\"audio\"][\"array\"]).numpy()\n",
    "\n",
    "input_values = processor(resampled_audio, return_tensors=\"pt\").input_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458eb943-dfae-4086-964b-b3c2c2a7f537",
   "metadata": {},
   "source": [
    "## Evaluate model (run forward pass)\n",
    "Here, we perform inference on the model with LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "55e85cbd-a506-424e-a0cc-043c6c75acb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u s air one four descend to flight level two nine zero']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run forward pass of the model\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "    \n",
    "# get the transcription with proce\n",
    "transcription = processor_with_lm.batch_decode(logits.numpy()).text\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ff593d17-cf4e-4ecd-bbee-10e8e045f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor_without_lm.batch_decode(pred_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42235af3-04b1-4004-a615-21f7459bf19d",
   "metadata": {},
   "source": [
    "That's all, If you have more question, reach us out on Github: https://github.com/idiap/w2v2-air-traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af53431-0e8e-4193-8a6e-e18418a38446",
   "metadata": {},
   "source": [
    "# Cite us\n",
    "If you use this code for your research, please cite our recent papers that involved this work:\n",
    "\n",
    "```\n",
    "@article{zuluaga2022bertraffic,\n",
    "    title={How Does Pre-trained Wav2Vec2. 0 Perform on Domain Shifted ASR? An Extensive Benchmark on Air Traffic Control Communications},\n",
    "    author={Zuluaga-Gomez, Juan and Prasad, Amrutha and Nigmatulina, Iuliia and Sarfjoo, Saeed and Motlicek, Petr and Kleinert, Matthias and Helmke, Hartmut and Ohneiser, Oliver and Zhan, Qingran},\n",
    "    journal={IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar},\n",
    "    year={2022}\n",
    "  }\n",
    "```\n",
    "\n",
    "and,\n",
    "\n",
    "```\n",
    "@article{zuluaga2022bertraffic,\n",
    "  title={BERTraffic: BERT-based Joint Speaker Role and Speaker Change Detection for Air Traffic Control Communications},\n",
    "  author={Zuluaga-Gomez, Juan and Sarfjoo, Seyyed Saeed and Prasad, Amrutha and Nigmatulina, Iuliia and Motlicek, Petr and Ondre, Karel and Ohneiser, Oliver and Helmke, Hartmut},\n",
    "  journal={IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar},\n",
    "  year={2022}\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca970f-5adb-4b68-a051-e397b680377e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
